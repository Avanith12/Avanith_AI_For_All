{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "089a7fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras import metrics, losses\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba061e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load both the teacher and student model\n",
    "scratch_student = load_model('student_model.h5')\n",
    "student_model = load_model('student_model.h5')\n",
    "teacher_model =  load_model('teacher_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a466c66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training images: (8000, 32, 32, 3), Training labels: (8000, 10)\n",
      "✅ Testing images: (2000, 32, 32, 3), Testing labels: (2000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Compute student model metrics without KD\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Dataset location\n",
    "data_dir = \"bing_images(10000)/raw\"\n",
    "img_height = 32\n",
    "img_width = 32\n",
    "\n",
    "# Sorted class names and label indices\n",
    "classes = sorted(os.listdir(data_dir))\n",
    "class_indices = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "X_train, y_train, X_test, y_test = [], [], [], []\n",
    "\n",
    "# Valid image extensions\n",
    "valid_exts = {'.jpg', '.jpeg', '.png', '.bmp', '.gif'}\n",
    "\n",
    "for cls in classes:\n",
    "    cls_path = os.path.join(data_dir, cls)\n",
    "\n",
    "    # Recursively find all valid image files\n",
    "    image_paths = []\n",
    "    for root, _, files in os.walk(cls_path):\n",
    "        for fname in files:\n",
    "            if os.path.splitext(fname)[1].lower() in valid_exts:\n",
    "                image_paths.append(os.path.join(root, fname))\n",
    "\n",
    "    if len(image_paths) < 1000:\n",
    "        print(f\"⚠️ Skipping '{cls}' — only {len(image_paths)} valid images found.\")\n",
    "        continue\n",
    "\n",
    "    selected = random.sample(image_paths, 1000)\n",
    "    train_imgs = selected[:800]\n",
    "    test_imgs = selected[800:1000]\n",
    "\n",
    "    for img_path in train_imgs:\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\").resize((img_width, img_height))\n",
    "            X_train.append(np.array(img))\n",
    "            y_train.append(class_indices[cls])\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Skipping train image: {img_path} - {e}\")\n",
    "\n",
    "    for img_path in test_imgs:\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\").resize((img_width, img_height))\n",
    "            X_test.append(np.array(img))\n",
    "            y_test.append(class_indices[cls])\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Skipping test image: {img_path} - {e}\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = np.array(X_train).astype('float32') / 255.0\n",
    "X_test = np.array(X_test).astype('float32') / 255.0\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# One-hot encode labels\n",
    "train_labels = to_categorical(y_train, num_classes=10)\n",
    "test_labels = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(f\"✅ Training images: {X_train.shape}, Training labels: {train_labels.shape}\")\n",
    "print(f\"✅ Testing images: {X_test.shape}, Testing labels: {test_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c16414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training images: (8000, 32, 32, 3), labels: (8000, 10)\n",
      "✅ Testing images: (2000, 32, 32, 3), labels: (2000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Compute student model metrics without KD\n",
    "\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# Dataset directory\n",
    "data_dir = \"bing_images(10000)/raw\"\n",
    "img_height = 32\n",
    "img_width = 32\n",
    "\n",
    "# Set up label mapping\n",
    "classes = sorted(os.listdir(data_dir))\n",
    "class_indices = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "# Initialize storage\n",
    "X_train, y_train, X_test, y_test = [], [], [], []\n",
    "\n",
    "# Valid extensions\n",
    "valid_exts = {'.jpg', '.jpeg', '.png', '.bmp', '.gif'}\n",
    "\n",
    "# Process each class\n",
    "for cls in classes:\n",
    "    cls_path = os.path.join(data_dir, cls)\n",
    "\n",
    "    # Recursively gather image files\n",
    "    image_paths = []\n",
    "    for root, _, files in os.walk(cls_path):\n",
    "        for fname in files:\n",
    "            if os.path.splitext(fname)[1].lower() in valid_exts:\n",
    "                image_paths.append(os.path.join(root, fname))\n",
    "\n",
    "    if len(image_paths) < 1000:\n",
    "        print(f\"⚠️ Skipping {cls}: only {len(image_paths)} images found\")\n",
    "        continue\n",
    "\n",
    "    selected = random.sample(image_paths, 1000)\n",
    "    train_imgs = selected[:800]\n",
    "    test_imgs = selected[800:1000]\n",
    "\n",
    "    for img_path in train_imgs:\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\").resize((img_width, img_height))\n",
    "            X_train.append(np.array(img))\n",
    "            y_train.append(class_indices[cls])\n",
    "        except:\n",
    "            print(f\"⚠️ Skipped train: {img_path}\")\n",
    "\n",
    "    for img_path in test_imgs:\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\").resize((img_width, img_height))\n",
    "            X_test.append(np.array(img))\n",
    "            y_test.append(class_indices[cls])\n",
    "        except:\n",
    "            print(f\"⚠️ Skipped test: {img_path}\")\n",
    "\n",
    "# Final conversions\n",
    "X_train = np.array(X_train).astype('float32') / 255.0\n",
    "X_test = np.array(X_test).astype('float32') / 255.0\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "train_labels = to_categorical(y_train, num_classes=10)\n",
    "test_labels = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(f\"✅ Training images: {X_train.shape}, labels: {train_labels.shape}\")\n",
    "print(f\"✅ Testing images: {X_test.shape}, labels: {test_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e467f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute student model metrics without KD\n",
    "\n",
    "scratch_student.compile(optimizer = 'sgd',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40c9f0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 74ms/step - accuracy: 0.1678 - loss: 2.9212\n",
      "Epoch 2/7\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.2614 - loss: 2.2264\n",
      "Epoch 3/7\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.2617 - loss: 2.0942\n",
      "Epoch 4/7\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 32ms/step - accuracy: 0.2916 - loss: 1.9985\n",
      "Epoch 5/7\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 32ms/step - accuracy: 0.3221 - loss: 1.9153\n",
      "Epoch 6/7\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.3293 - loss: 1.8870\n",
      "Epoch 7/7\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.3533 - loss: 1.8425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b167ee8d30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let us try to see what if we directly train the student model without using knowledge distillation\n",
    "\n",
    "scratch_student.fit(X_train, train_labels, epochs=7, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28dbb572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.3194 - loss: 1.8893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8287723064422607, 0.3644999861717224]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We evaluate student model for its loss and accuracy, if the student model is trained without using knowledge distillation\n",
    "\n",
    "scratch_student.evaluate(X_test, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edd31187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us try using knowledge distillation\n",
    "# KNOWLEDGE DISTILLATION CLASS, You can adjust alpha based on how much you want the student to learn from the teacher\n",
    "\n",
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.2,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\"Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super().compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def compute_loss(\n",
    "        self, x=None, y=None, y_pred=None, sample_weight=None, allow_empty=False\n",
    "    ):\n",
    "        teacher_pred = self.teacher(x, training=False)\n",
    "        student_loss = self.student_loss_fn(y, y_pred)\n",
    "\n",
    "        distillation_loss = self.distillation_loss_fn(\n",
    "            tf.nn.softmax(teacher_pred / self.temperature, axis=1),\n",
    "            tf.nn.softmax(y_pred / self.temperature, axis=1),\n",
    "        ) * (self.temperature**2)\n",
    "\n",
    "        loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "        return loss\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.student(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71989945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - categorical_accuracy: 0.1679 - loss: 2.2472 - val_categorical_accuracy: 0.0000e+00 - val_loss: 3.2024\n",
      "Epoch 2/7\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - categorical_accuracy: 0.2946 - loss: 2.2002 - val_categorical_accuracy: 0.0000e+00 - val_loss: 3.9436\n",
      "Epoch 3/7\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step - categorical_accuracy: 0.3559 - loss: 2.1803 - val_categorical_accuracy: 0.0000e+00 - val_loss: 3.8906\n",
      "Epoch 4/7\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - categorical_accuracy: 0.4252 - loss: 2.1519 - val_categorical_accuracy: 0.0000e+00 - val_loss: 3.9502\n",
      "Epoch 5/7\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step - categorical_accuracy: 0.4867 - loss: 2.1296 - val_categorical_accuracy: 0.0000e+00 - val_loss: 4.2124\n",
      "Epoch 6/7\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step - categorical_accuracy: 0.5642 - loss: 2.0931 - val_categorical_accuracy: 0.0000e+00 - val_loss: 4.3568\n",
      "Epoch 7/7\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 37ms/step - categorical_accuracy: 0.6489 - loss: 2.0571 - val_categorical_accuracy: 0.0000e+00 - val_loss: 4.8269\n"
     ]
    }
   ],
   "source": [
    "# Initialize the distiller\n",
    "# Train the student model using knowledge distillation\n",
    "\n",
    "distiller = Distiller(student=student_model, teacher=teacher_model)\n",
    "\n",
    "# Compiling the Distiller. You can adjust alpha based on how much you want the student to learn from the teacher\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[metrics.CategoricalAccuracy()],\n",
    "    student_loss_fn=losses.CategoricalCrossentropy(),\n",
    "    distillation_loss_fn=losses.CategoricalCrossentropy(),\n",
    "    alpha=0.2,\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "# Fitting the student model receiving KD\n",
    "history = distiller.fit(\n",
    "    X_train,\n",
    "    train_labels,\n",
    "    epochs=7,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5ce326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - categorical_accuracy: 0.4421 - loss: 2.2550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.7153728008270264, 0.3695000112056732]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We evaluate student model again for its loss and accuracy,\n",
    "# But this time the student model is trained using knowledge distillation\n",
    "# You can compare this results with the results above\n",
    "\n",
    "distiller.evaluate(X_test, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe77cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
